{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431cbdf6",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79b4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn functions\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc739d55",
   "metadata": {},
   "source": [
    "|Matric|Use case|\n",
    "|----|----|\n",
    "|**Accurecy**||\n",
    "|**Precision**||\n",
    "|**Recall**||\n",
    "|**F1**||\n",
    "|**Misclassification**||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78cf64f",
   "metadata": {},
   "source": [
    "- **Accuracy**: currectly predicted instances over all the the instances together.\n",
    "- **Precision**: % of all correctly identified positives out of all the positive instances. $\\frac{TP}{TP + FP}$\n",
    "    - out of all everyting that my modely labeled positive, how many of them are actualy positive\n",
    "- **Recall**: The percentage of all corretly identified positive instances out of all the positive instances. $\\frac{TP}{TP + FN}$\n",
    "    - Out of everythink I labeled positive, how many of them was I was able to capture\n",
    "\n",
    "2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69579364",
   "metadata": {},
   "source": [
    "- In the context of this problem, what is a false positive?\n",
    "    - Predicted dog when it actualy a cat\n",
    "\n",
    "- In the context of this problem, what is a false negative?\n",
    "    - predicted a cat when it's actualy a dog\n",
    "- How would you describe this model?\n",
    "    - Dog prediction model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68af17",
   "metadata": {},
   "source": [
    "**3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.**\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "- **An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d4fb2",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "||Senerio|\n",
    "|----|----|\n",
    "|**Positive case**|Identify ducks that are defects|\n",
    "|**Negative case**|Identify ducks that are NOT defects|\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "|Actual|Predicted|possible outcome|\n",
    "|----|----|----|\n",
    "|True (pos case)|Positive| Model predict's a defected duck, and actual duck is a defect|\n",
    "|True (pos case)|Negative| Model predict's NON defected duck, and actual duck is a defect|\n",
    "|False (neg case)|Positive| Model predict's a defected duck, and actual is a NON defected duck|\n",
    "|False (neg case)|Negative|Model predict's a NON defected duck, and actual is a NON defected duck|\n",
    "\n",
    "What are the cost of the outcomes?\n",
    "- $FP$: The manufacturing team receive a defected rubber duck, when then wanted a good duck\n",
    "- $FN$: The manufacturing team recieve a good duck when the wanted to get a broken duck.\n",
    "\n",
    "Which metric should we use?\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d77f1",
   "metadata": {},
   "source": [
    "- **Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5a34b",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "- $Pos$: Correctly identify all the rubber ducks that have a defect.\n",
    "- $Neg$: All the identified rubber ducks are not defects.\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "- $TP$: Model identifies a defected rubber duck, and the actual result is a defected rubber duck.\n",
    "- $FP$: Mode identifies NON-defected rubber duck, and the actual result is a defected rubber duck.\n",
    "- $FN$: Model identifies a defected rubber duck, The actual result is a NON-defected rubber duck\n",
    "- $TN$: Model identifies a NON-defected rubber duck,and the actual result is a NON-defected rubber duck.\n",
    "\n",
    "What are the cost of the outcomes?\n",
    "- $FP$: Customer receive vacation to Hawaii when and they received a defected rubber duck.\n",
    "- $FN$: Customer receive vacation to Hawaii when and they didn't receive a defected rubber duck.\n",
    "\n",
    "Which metric should we use?\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403ca19",
   "metadata": {},
   "source": [
    "4. You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "- In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3837edc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"gives_you_paws.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080b9e6",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "- $Pos$: Model correctly identifies dogs or cats\n",
    "- $Neg$: Model doesn't identify dogs or cats correctly.\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "- $TP$: Model correctly identify a dog and it's actualy a dog.\n",
    "- $FP$: Model predict a cat, when it's actualy a dog. \n",
    "- $FN$: Modely predict a cat when it's actualy a cat.\n",
    "- $TN$: Model predict dog, when it's actualy a cat.\n",
    "\n",
    "What are the cost of the outcomes?\n",
    "- $FP$: Customer receives a picture of a cat when the wanted a dog\n",
    "- $FN$: Customer receive va picture of a cat when the wanted a cat.\n",
    "\n",
    "Which metric should we use?\n",
    "- accuracy\n",
    "\n",
    "### Accuracy\n",
    "**Manual hard code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefe9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 dog prediction accuracy: 0.5228\n",
      "model2 dog prediction accuracy: 0.3194\n",
      "model3 dog prediction accuracy: 0.331\n",
      "model4 dog prediction accuracy: 0.622\n"
     ]
    }
   ],
   "source": [
    "# dog accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    acc =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()/len(df.actual)\n",
    "    print(f\"{i} dog prediction accuracy:\", acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672a2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 cat prediction accuracy: 0.2846\n",
      "model2 cat prediction accuracy: 0.311\n",
      "model3 cat prediction accuracy: 0.1786\n",
      "model4 cat prediction accuracy: 0.1206\n"
     ]
    }
   ],
   "source": [
    "# cat accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    acc =((df.actual == \"cat\") & (df[i] == \"cat\")).sum()/len(df.actual)\n",
    "    print(f\"{i} cat prediction accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cc8ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy score: 0.8074000000000001\n",
      "model2 accuracy score: 0.6304000000000001\n",
      "model3 accuracy score: 0.5096\n",
      "model4 accuracy score: 0.7426\n"
     ]
    }
   ],
   "source": [
    "# full acuracy score\n",
    "# dog accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # combing the dog and cate accuracies\n",
    "    dog_acc =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()/len(df.actual)\n",
    "    cat_acc =((df.actual == \"cat\") & (df[i] == \"cat\")).sum()/len(df.actual)\n",
    "    print(f\"{i} accuracy score:\", dog_acc + cat_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cac4e",
   "metadata": {},
   "source": [
    "- Model 1 is really good at predicting dogs\n",
    "\n",
    "**using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "924f6d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy score: 0.8074\n",
      "model2 accuracy score: 0.6304\n",
      "model3 accuracy score: 0.5096\n",
      "model4 accuracy score: 0.7426\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns[1:]:\n",
    "    #                actual , predicted by model\n",
    "    acc = accuracy_score(df.actual, df[i])\n",
    "    print(f\"{i} accuracy score:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4429d",
   "metadata": {},
   "source": [
    "**Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?**\n",
    "\n",
    "\n",
    "### Precision\n",
    "**Manual hard code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d34190df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision score: 0.803318992009834\n",
      "model2 precision score: 0.49078057775046097\n",
      "model3 precision score: 0.5086047940995697\n",
      "model4 precision score: 0.9557467732022127\n"
     ]
    }
   ],
   "source": [
    "# precission\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    tp =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()\n",
    "    fp = ((df.actual == \"dog\") & (df[i] == \"cat\")).sum()\n",
    "    press = tp/(tp + fp)\n",
    "    print(f\"{i} precision score:\", press)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab60e05",
   "metadata": {},
   "source": [
    "**Using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e7b2503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision score: 0.8900238338440586\n",
      "model2 precision score: 0.8931767337807607\n",
      "model3 precision score: 0.6598883572567783\n",
      "model4 precision score: 0.7312485304490948\n"
     ]
    }
   ],
   "source": [
    "# precision_score\n",
    "for i in df.columns[1:]:\n",
    "    #                actual , predicted by model\n",
    "    press = precision_score(df.actual, df[i], pos_label=\"dog\")\n",
    "    print(f\"{i} precision score:\", press)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4e782",
   "metadata": {},
   "source": [
    "- Model 1 and model 4 are really good on predicting dog\n",
    "\n",
    "**Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?**\n",
    "\n",
    "### Recall\n",
    "\n",
    "**Manual hard cod**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0faac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall score: 0.803318992009834\n",
      "model2 recall score: 0.49078057775046097\n",
      "model3 recall score: 0.5086047940995697\n",
      "model4 recall score: 0.9557467732022127\n"
     ]
    }
   ],
   "source": [
    "# recall\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    tp =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()\n",
    "    fn = ((df.actual == \"dog\") & (df[i] == \"cat\")).sum()\n",
    "    press = tp/(tp + fn)\n",
    "    print(f\"{i} recall score:\", press)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b92c49",
   "metadata": {},
   "source": [
    "**Using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ef87631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model1', 'model2', 'model3', 'model4'], dtype='object')\n",
      "model1 precision score: 0.803318992009834\n",
      "model2 precision score: 0.49078057775046097\n",
      "model3 precision score: 0.5086047940995697\n",
      "model4 precision score: 0.9557467732022127\n"
     ]
    }
   ],
   "source": [
    "# precision_score\n",
    "\n",
    "print(df.columns[1:])\n",
    "for i in df.columns[1:]:\n",
    "    #        actual , predicted by model\n",
    "    press = recall_score(df.actual, df[i], pos_label=\"dog\")\n",
    "    print(f\"{i} precision score:\", press)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fc632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
