{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e5b8a9",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51b688b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# import sklearn functions\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715af37",
   "metadata": {},
   "source": [
    "- **Accuracy**: currectly predicted instances over all the the instances together.\n",
    "- **Precision**: % of all correctly identified positives out of all the positive instances. $\\frac{TP}{TP + FP}$\n",
    "    - out of all everyting that my modely labeled positive, how many of them are actualy positive\n",
    "- **Recall**: The percentage of all corretly identified positive instances out of all the positive instances. $\\frac{TP}{TP + FN}$\n",
    "    - Out of everythink I labeled positive, how many of them was I was able to capture\n",
    "\n",
    "2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac5a1f",
   "metadata": {},
   "source": [
    "- In the context of this problem, what is a false positive?\n",
    "    - Predicted dog when it actualy a cat\n",
    "\n",
    "- In the context of this problem, what is a false negative?\n",
    "    - predicted a cat when it's actualy a dog\n",
    "- How would you describe this model?\n",
    "    - Dog prediction model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ecc51",
   "metadata": {},
   "source": [
    "**3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.**\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "**An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c07b1",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "||Senerio|\n",
    "|----|----|\n",
    "|**Positive case**|Identify ducks that are defects|\n",
    "|**Negative case**|Identify ducks that are NOT defects|\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "|Actual|Predicted|possible outcome|\n",
    "|----|----|----|\n",
    "|True (pos case)|Positive| Model predicted a defect, and actual is defect|\n",
    "|True (pos case)|Negative|Model predict a NON defect, and actual is a NON defect|\n",
    "|False (neg case)|Positive|Model predicted a defect, and actual is a NON defect|\n",
    "|False (neg case)|Negative|Model predicted a NON defect, and actial is a defect|\n",
    "\n",
    "- Questoin to ask to correctly identify the decision matic\n",
    "    - How can I make the predicted result true or fase?\n",
    "    \n",
    "What are the cost of the outcomes?\n",
    "||Cost of outcome|\n",
    "|----|----|\n",
    "|**False Positive**|The manufacturing team receive a NON defected rubber duck, when they were expecting a defected duck|\n",
    "|**False Negative**|The manufacturing team receive a defected rubber duck, when they were expecting a NON defected duck|\n",
    "\n",
    "Which metric should we use?\n",
    "- Because the false-negative if more harm full to the manufatureing team, we use `Recall_score`\n",
    "    - Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29777987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = np.random.choice([\"defect\", \"non-defect\"],60)\n",
    "pred = np.random.choice([\"defect\",\"non-defect\"],60)\n",
    "\n",
    "# using the recall from from sklearn\n",
    "metrics.recall_score(act,pred, pos_label=\"defect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5fe05",
   "metadata": {},
   "source": [
    "**Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71c2ad",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "||Senerio|\n",
    "|----|----|\n",
    "|**Positive case**|Identify ducks that are defects|\n",
    "|**Negative case**|Identify ducks that are NOT defects|\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "|Actual|Predicted|possible outcome|\n",
    "|----|----|----|\n",
    "|True (pos case)|Positive| Model predicted a defect, and actual is defect|\n",
    "|True (pos case)|Negative|Model predict a NON defect, and actual is a NON defect|\n",
    "|False (neg case)|Positive|Model predicted a defect, and actual is a NON defect|\n",
    "|False (neg case)|Negative|Model predicted a NON defect, and actial is a defect|\n",
    "\n",
    "- Questoin to ask to correctly identify the decision matic\n",
    "    - How can I make the predicted result true or fase?\n",
    "    \n",
    "What are the cost of the outcomes?\n",
    "||Cost of outcome|\n",
    "|----|----|\n",
    "|**False Positive**|The PR team give a vaccation package to a NON defected rubber duck customer|\n",
    "|**False Negative**|The PR team give a vaccation package to a defected rubber duck customer|\n",
    "\n",
    "Which metric should we use?\n",
    "- Because the false-positive is more harmful to the PR team, we use `Precision_scare`\n",
    "    - Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa4779a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = np.random.choice([\"defect\", \"non-defect\"],60)\n",
    "pred = np.random.choice([\"defect\",\"non-defect\"],60)\n",
    "\n",
    "# using the recall from from sklearn\n",
    "metrics.precision_score(act,pred, pos_label=\"defect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dec72b",
   "metadata": {},
   "source": [
    "**4. You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).**\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "- In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f132e259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"gives_you_paws.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7003334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      cat\n",
       "1    dog    dog    cat    cat    dog      cat\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      cat\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a baseline model\n",
    "df[\"baseline\"]= np.random.choice([\"cat\",\"dog\"],len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58e95d",
   "metadata": {},
   "source": [
    "What is the positive and negative case?\n",
    "||Senerio|\n",
    "|----|----|\n",
    "|**Positive case**|Model correctly identifies dogs|\n",
    "|**Negative case**|Model correctly identifies cats|\n",
    "\n",
    "Whant are the possible outcomes?\n",
    "\n",
    "|Actual|Predicted|possible outcome|\n",
    "|----|----|----|\n",
    "|True (pos case)|Positive| Model predict's dog, and actual is dog|\n",
    "|True (pos case)|Negative|Model predict's cat, and actual is cat|\n",
    "|False (neg case)|Positive|Model predict's dog, and actual is cat|\n",
    "|False (neg case)|Negative|Model predict's cat, and actial is dog|\n",
    "\n",
    "- Questoin to ask to correctly identify the decision matic\n",
    "    - How can I make the predicted result true or fase?\n",
    "\n",
    "What are the cost of the outcomes?\n",
    "\n",
    "||Cost of outcome|\n",
    "|----|----|\n",
    "|**False Positive**|Give and paw deliver a cat image when the user requested a dog|\n",
    "|**False Negative**|Give and paw deliver a dog image when the user requested a cat|\n",
    "\n",
    "Which metric should we use?\n",
    "- accuracy\n",
    "- F1 might also be usefull here\n",
    "\n",
    "### Accuracy\n",
    "**Manual hard code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f70e2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 dog prediction accuracy: 0.5228\n",
      "model2 dog prediction accuracy: 0.3194\n",
      "model3 dog prediction accuracy: 0.331\n",
      "model4 dog prediction accuracy: 0.622\n",
      "baseline dog prediction accuracy: 0.3288\n"
     ]
    }
   ],
   "source": [
    "# dog accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    acc =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()/len(df.actual)\n",
    "    print(f\"{i} dog prediction accuracy:\", acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69ee88df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 cat prediction accuracy: 0.2846\n",
      "model2 cat prediction accuracy: 0.311\n",
      "model3 cat prediction accuracy: 0.1786\n",
      "model4 cat prediction accuracy: 0.1206\n",
      "baseline cat prediction accuracy: 0.1812\n"
     ]
    }
   ],
   "source": [
    "# cat accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    acc =((df.actual == \"cat\") & (df[i] == \"cat\")).sum()/len(df.actual)\n",
    "    print(f\"{i} cat prediction accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7ef289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy score: 0.8074000000000001\n",
      "model2 accuracy score: 0.6304000000000001\n",
      "model3 accuracy score: 0.5096\n",
      "model4 accuracy score: 0.7426\n",
      "baseline accuracy score: 0.51\n"
     ]
    }
   ],
   "source": [
    "# full acuracy score\n",
    "# dog accuracy test\n",
    "for i in df.columns[1:]:\n",
    "    # combing the dog and cate accuracies\n",
    "    dog_acc =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()/len(df.actual)\n",
    "    cat_acc =((df.actual == \"cat\") & (df[i] == \"cat\")).sum()/len(df.actual)\n",
    "    print(f\"{i} accuracy score:\", dog_acc + cat_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ce42d",
   "metadata": {},
   "source": [
    "- Model 1 is really good at predicting dogs\n",
    "\n",
    "**using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e17e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy score: 0.8074\n",
      "model2 accuracy score: 0.6304\n",
      "model3 accuracy score: 0.5096\n",
      "model4 accuracy score: 0.7426\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns[1:]:\n",
    "    #                actual , predicted by model\n",
    "    acc = accuracy_score(df.actual, df[i])\n",
    "    print(f\"{i} accuracy score:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16f52d",
   "metadata": {},
   "source": [
    "**Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?**\n",
    "\n",
    "\n",
    "### Precision\n",
    "**Manual hard code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08d10732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision score: 0.8900238338440586\n",
      "model2 precision score: 0.8931767337807607\n",
      "model3 precision score: 0.6598883572567783\n",
      "model4 precision score: 0.7312485304490948\n",
      "baseline precision score: 0.6618357487922706\n"
     ]
    }
   ],
   "source": [
    "# precission\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    tp =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()\n",
    "    fp = ((df.actual == \"cat\") & (df[i] == \"dog\")).sum()\n",
    "    press = tp/(tp + fp)\n",
    "    print(f\"{i} precision score:\", press)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f857",
   "metadata": {},
   "source": [
    "**Using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e9b3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision score: 0.8900238338440586\n",
      "model2 precision score: 0.8931767337807607\n",
      "model3 precision score: 0.6598883572567783\n",
      "model4 precision score: 0.7312485304490948\n"
     ]
    }
   ],
   "source": [
    "# precision_score\n",
    "for i in df.columns[1:]:\n",
    "    #                actual , predicted by model\n",
    "    press = precision_score(df.actual, df[i], pos_label=\"dog\")\n",
    "    print(f\"{i} precision score:\", press)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617eb25",
   "metadata": {},
   "source": [
    "- Model 1 and model 4 are really good on predicting dog\n",
    "\n",
    "**Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?**\n",
    "\n",
    "### Recall\n",
    "\n",
    "**Manual hard cod**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b03f4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall score: 0.803318992009834\n",
      "model2 recall score: 0.49078057775046097\n",
      "model3 recall score: 0.5086047940995697\n",
      "model4 recall score: 0.9557467732022127\n",
      "baseline recall score: 0.5052243392747388\n"
     ]
    }
   ],
   "source": [
    "# recall\n",
    "for i in df.columns[1:]:\n",
    "    # currectly predicted instances over all the the instances together.\n",
    "    tp =((df.actual == \"dog\") & (df[i] == \"dog\")).sum()\n",
    "    fn = ((df.actual == \"dog\") & (df[i] == \"cat\")).sum()\n",
    "    press = tp/(tp + fn)\n",
    "    print(f\"{i} recall score:\", press)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4a037",
   "metadata": {},
   "source": [
    "**Using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afeeaba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model1', 'model2', 'model3', 'model4', 'baseline'], dtype='object')\n",
      "model1 precision score: 0.803318992009834\n",
      "model2 precision score: 0.49078057775046097\n",
      "model3 precision score: 0.5086047940995697\n",
      "model4 precision score: 0.9557467732022127\n",
      "baseline precision score: 0.5052243392747388\n"
     ]
    }
   ],
   "source": [
    "# precision_score\n",
    "\n",
    "print(df.columns[1:])\n",
    "for i in df.columns[1:]:\n",
    "    #        actual , predicted by model\n",
    "    press = recall_score(df.actual, df[i], pos_label=\"dog\")\n",
    "    print(f\"{i} precision score:\", press)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354a16e",
   "metadata": {},
   "source": [
    "More metric functions from sklearn\n",
    "- sklearn.metrics.accuracy_score\n",
    "- sklearn.metrics.precision_score\n",
    "- sklearn.metrics.recall_score\n",
    "- sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64fecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
